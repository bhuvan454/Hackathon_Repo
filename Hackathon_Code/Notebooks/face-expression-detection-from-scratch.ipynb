{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip uninstall keras -y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install keras==2.7.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install --upgrade pip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display some images for every different expression\n\nimport numpy as np\nimport seaborn as sns\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nimport matplotlib.pyplot as plt\nimport os\n\n# size of the image: 48*48 pixels\npic_size = 48\n\n# input path for the images\nbase_path = \"../input/simple-face-expressions/face_expression_simple\"\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-10T00:53:51.793360Z","iopub.execute_input":"2021-11-10T00:53:51.793683Z","iopub.status.idle":"2021-11-10T00:53:53.658651Z","shell.execute_reply.started":"2021-11-10T00:53:51.793655Z","shell.execute_reply":"2021-11-10T00:53:53.657855Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Image augmentation using keras ImageDataGenerator","metadata":{}},{"cell_type":"markdown","source":"# Defining our 4 Convolution and 2 Dense layers model","metadata":{}},{"cell_type":"code","source":"# modeling\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model,Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Dense,Flatten\n\nfrom tensorflow.keras.applications import resnet\nfrom tensorflow.keras.applications.resnet50 import preprocess_input\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.callbacks import EarlyStopping,LearningRateScheduler\n\n\nfrom tensorflow.keras import backend as K\nK.clear_session()","metadata":{"execution":{"iopub.status.busy":"2021-11-10T00:53:58.993648Z","iopub.execute_input":"2021-11-10T00:53:58.993997Z","iopub.status.idle":"2021-11-10T00:53:59.004368Z","shell.execute_reply.started":"2021-11-10T00:53:58.993969Z","shell.execute_reply":"2021-11-10T00:53:59.003368Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# building data generator \n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nbatch_size = 32\nbase_path = \"../input/simple-face-expressions/face_expression_simple/\"\n\n\n\ntrain_datagen = ImageDataGenerator()\n#                                   rescale = 1.0/255.0,\n#                                   width_shift_range = 0.1,\n#                                    height_shift_range = 0.1,\n#                                    rotation_range = 20,\n#                                    horizontal_flip = True)\n\nvalidation_datagen = ImageDataGenerator()\n\ntrain_generator = train_datagen.flow_from_directory(base_path + \"train\",\n                                                    target_size=(56,56),\n                                                    batch_size=batch_size,\n                                                    class_mode='categorical',\n                                                    shuffle=True)\n\nvalidation_generator = validation_datagen.flow_from_directory(base_path + \"validation\",\n                                                    target_size=(56,56),\n                                                    batch_size=batch_size,\n                                                    class_mode='categorical',\n                                                    shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-10T00:55:08.714006Z","iopub.execute_input":"2021-11-10T00:55:08.714381Z","iopub.status.idle":"2021-11-10T00:55:19.868562Z","shell.execute_reply.started":"2021-11-10T00:55:08.714351Z","shell.execute_reply":"2021-11-10T00:55:19.867492Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Found 27236 images belonging to 3 classes.\nFound 6955 images belonging to 3 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.applications import resnet\n#loading resent \nresNet =         resnet.ResNet50(weights = 'imagenet',\n                        include_top = False,\n                        input_shape = (56,56, 3))\n\nresNet.trainable = False # Freeze layers\nresNet_model = Sequential([\n        resNet,\n        Flatten(),\n        Dense(1024, activation = 'relu'),\n        Dropout(0.5),\n        Dense(3, activation = 'softmax')])\n     \n\noptimizer = optimizers.Adam(1e-5)","metadata":{"execution":{"iopub.status.busy":"2021-11-10T00:55:51.907780Z","iopub.execute_input":"2021-11-10T00:55:51.908126Z","iopub.status.idle":"2021-11-10T00:55:56.474399Z","shell.execute_reply.started":"2021-11-10T00:55:51.908084Z","shell.execute_reply":"2021-11-10T00:55:56.473487Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"epoch = 50\nlearning_rate = 3e-5 \nlr_start = 0.00000001\nlr_min = 0.000001\nlr_max = 3e-5 \nlr_rampup_epochs = 1\nlr_sustain_epochs = 1\nlr_exp_decay = .8\n\ndef lrfn(epoch):\n    if epoch < lr_rampup_epochs:\n        lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start\n    elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n        lr = lr_max\n    else:\n        lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n    return lr\n\n\n# checkpoint to save best model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.python.keras.callbacks import CallbackList \n\ncheckpoint = ModelCheckpoint(\"model_weights.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n\nearlystop = EarlyStopping(patience= 5)\n    \nlr_callback = LearningRateScheduler(lrfn, verbose = True)\n\ncallbacks_list =  [checkpoint,earlystop, lr_callback]","metadata":{"execution":{"iopub.status.busy":"2021-11-10T00:55:56.477994Z","iopub.execute_input":"2021-11-10T00:55:56.478354Z","iopub.status.idle":"2021-11-10T00:55:56.488776Z","shell.execute_reply.started":"2021-11-10T00:55:56.478321Z","shell.execute_reply":"2021-11-10T00:55:56.487911Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"resNet_model.compile(optimizer = optimizer,\n             loss = 'categorical_crossentropy',\n             metrics = ['accuracy'])\n\n\n# number of epochs to train the NN\nepochs = 50\n\n\nresnet_history = resNet_model.fit_generator(generator=train_generator,\n                                steps_per_epoch=train_generator.n//train_generator.batch_size,\n                                epochs=epochs,\n                                validation_data = validation_generator,\n                                validation_steps = validation_generator.n//validation_generator.batch_size,\n                                callbacks= callbacks_list)","metadata":{"execution":{"iopub.status.busy":"2021-11-10T00:58:19.469864Z","iopub.execute_input":"2021-11-10T00:58:19.470239Z","iopub.status.idle":"2021-11-10T01:06:17.673324Z","shell.execute_reply.started":"2021-11-10T00:58:19.470204Z","shell.execute_reply":"2021-11-10T01:06:17.672609Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"\nEpoch 00001: LearningRateScheduler reducing learning rate to 1e-08.\nEpoch 1/50\n851/851 [==============================] - ETA: 0s - loss: 0.9845 - accuracy: 0.5719\nEpoch 00001: val_accuracy improved from 0.57892 to 0.59188, saving model to model_weights.h5\n851/851 [==============================] - 33s 39ms/step - loss: 0.9845 - accuracy: 0.5719 - val_loss: 0.9131 - val_accuracy: 0.5919 - lr: 1.0000e-08\n\nEpoch 00002: LearningRateScheduler reducing learning rate to 3e-05.\nEpoch 2/50\n851/851 [==============================] - ETA: 0s - loss: 0.9348 - accuracy: 0.5824\nEpoch 00002: val_accuracy improved from 0.59188 to 0.61550, saving model to model_weights.h5\n851/851 [==============================] - 32s 37ms/step - loss: 0.9348 - accuracy: 0.5824 - val_loss: 0.8613 - val_accuracy: 0.6155 - lr: 3.0000e-05\n\nEpoch 00003: LearningRateScheduler reducing learning rate to 3e-05.\nEpoch 3/50\n850/851 [============================>.] - ETA: 0s - loss: 0.8011 - accuracy: 0.6437\nEpoch 00003: val_accuracy improved from 0.61550 to 0.63033, saving model to model_weights.h5\n851/851 [==============================] - 50s 59ms/step - loss: 0.8009 - accuracy: 0.6437 - val_loss: 0.8402 - val_accuracy: 0.6303 - lr: 3.0000e-05\n\nEpoch 00004: LearningRateScheduler reducing learning rate to 2.4200000000000002e-05.\nEpoch 4/50\n851/851 [==============================] - ETA: 0s - loss: 0.7313 - accuracy: 0.6783\nEpoch 00004: val_accuracy improved from 0.63033 to 0.64099, saving model to model_weights.h5\n851/851 [==============================] - 38s 45ms/step - loss: 0.7313 - accuracy: 0.6783 - val_loss: 0.8317 - val_accuracy: 0.6410 - lr: 2.4200e-05\n\nEpoch 00005: LearningRateScheduler reducing learning rate to 1.9560000000000006e-05.\nEpoch 5/50\n851/851 [==============================] - ETA: 0s - loss: 0.6685 - accuracy: 0.7100\nEpoch 00005: val_accuracy improved from 0.64099 to 0.64963, saving model to model_weights.h5\n851/851 [==============================] - 27s 32ms/step - loss: 0.6685 - accuracy: 0.7100 - val_loss: 0.8160 - val_accuracy: 0.6496 - lr: 1.9560e-05\n\nEpoch 00006: LearningRateScheduler reducing learning rate to 1.5848000000000005e-05.\nEpoch 6/50\n850/851 [============================>.] - ETA: 0s - loss: 0.6089 - accuracy: 0.7445\nEpoch 00006: val_accuracy did not improve from 0.64963\n851/851 [==============================] - 26s 31ms/step - loss: 0.6089 - accuracy: 0.7445 - val_loss: 0.8190 - val_accuracy: 0.6470 - lr: 1.5848e-05\n\nEpoch 00007: LearningRateScheduler reducing learning rate to 1.2878400000000004e-05.\nEpoch 7/50\n850/851 [============================>.] - ETA: 0s - loss: 0.5659 - accuracy: 0.7664\nEpoch 00007: val_accuracy improved from 0.64963 to 0.65323, saving model to model_weights.h5\n851/851 [==============================] - 27s 31ms/step - loss: 0.5660 - accuracy: 0.7663 - val_loss: 0.8145 - val_accuracy: 0.6532 - lr: 1.2878e-05\n\nEpoch 00008: LearningRateScheduler reducing learning rate to 1.0502720000000003e-05.\nEpoch 8/50\n850/851 [============================>.] - ETA: 0s - loss: 0.5323 - accuracy: 0.7883\nEpoch 00008: val_accuracy improved from 0.65323 to 0.65481, saving model to model_weights.h5\n851/851 [==============================] - 27s 32ms/step - loss: 0.5322 - accuracy: 0.7883 - val_loss: 0.8145 - val_accuracy: 0.6548 - lr: 1.0503e-05\n\nEpoch 00009: LearningRateScheduler reducing learning rate to 8.602176000000003e-06.\nEpoch 9/50\n849/851 [============================>.] - ETA: 0s - loss: 0.4984 - accuracy: 0.8052\nEpoch 00009: val_accuracy improved from 0.65481 to 0.65913, saving model to model_weights.h5\n851/851 [==============================] - 27s 32ms/step - loss: 0.4984 - accuracy: 0.8052 - val_loss: 0.8063 - val_accuracy: 0.6591 - lr: 8.6022e-06\n\nEpoch 00010: LearningRateScheduler reducing learning rate to 7.081740800000002e-06.\nEpoch 10/50\n850/851 [============================>.] - ETA: 0s - loss: 0.4780 - accuracy: 0.8136\nEpoch 00010: val_accuracy improved from 0.65913 to 0.66100, saving model to model_weights.h5\n851/851 [==============================] - 27s 32ms/step - loss: 0.4780 - accuracy: 0.8137 - val_loss: 0.8067 - val_accuracy: 0.6610 - lr: 7.0817e-06\n\nEpoch 00011: LearningRateScheduler reducing learning rate to 5.865392640000002e-06.\nEpoch 11/50\n851/851 [==============================] - ETA: 0s - loss: 0.4568 - accuracy: 0.8267\nEpoch 00011: val_accuracy improved from 0.66100 to 0.66777, saving model to model_weights.h5\n851/851 [==============================] - 28s 33ms/step - loss: 0.4568 - accuracy: 0.8267 - val_loss: 0.7985 - val_accuracy: 0.6678 - lr: 5.8654e-06\n\nEpoch 00012: LearningRateScheduler reducing learning rate to 4.892314112000002e-06.\nEpoch 12/50\n851/851 [==============================] - ETA: 0s - loss: 0.4435 - accuracy: 0.8324\nEpoch 00012: val_accuracy improved from 0.66777 to 0.66849, saving model to model_weights.h5\n851/851 [==============================] - 27s 32ms/step - loss: 0.4435 - accuracy: 0.8324 - val_loss: 0.8004 - val_accuracy: 0.6685 - lr: 4.8923e-06\n\nEpoch 00013: LearningRateScheduler reducing learning rate to 4.113851289600002e-06.\nEpoch 13/50\n849/851 [============================>.] - ETA: 0s - loss: 0.4307 - accuracy: 0.8396\nEpoch 00013: val_accuracy did not improve from 0.66849\n851/851 [==============================] - 26s 31ms/step - loss: 0.4306 - accuracy: 0.8396 - val_loss: 0.8026 - val_accuracy: 0.6656 - lr: 4.1139e-06\n\nEpoch 00014: LearningRateScheduler reducing learning rate to 3.4910810316800013e-06.\nEpoch 14/50\n850/851 [============================>.] - ETA: 0s - loss: 0.4159 - accuracy: 0.8487\nEpoch 00014: val_accuracy did not improve from 0.66849\n851/851 [==============================] - 27s 31ms/step - loss: 0.4159 - accuracy: 0.8487 - val_loss: 0.8014 - val_accuracy: 0.6672 - lr: 3.4911e-06\n\nEpoch 00015: LearningRateScheduler reducing learning rate to 2.9928648253440008e-06.\nEpoch 15/50\n851/851 [==============================] - ETA: 0s - loss: 0.4057 - accuracy: 0.8538\nEpoch 00015: val_accuracy did not improve from 0.66849\n851/851 [==============================] - 26s 31ms/step - loss: 0.4057 - accuracy: 0.8538 - val_loss: 0.8015 - val_accuracy: 0.6646 - lr: 2.9929e-06\n\nEpoch 00016: LearningRateScheduler reducing learning rate to 2.594291860275201e-06.\nEpoch 16/50\n850/851 [============================>.] - ETA: 0s - loss: 0.3983 - accuracy: 0.8597\nEpoch 00016: val_accuracy did not improve from 0.66849\n851/851 [==============================] - 26s 30ms/step - loss: 0.3983 - accuracy: 0.8598 - val_loss: 0.8040 - val_accuracy: 0.6663 - lr: 2.5943e-06\n","output_type":"stream"}]},{"cell_type":"code","source":"resNet_model.save(\"face_resnet_model.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-11-10T01:08:16.315194Z","iopub.execute_input":"2021-11-10T01:08:16.315539Z","iopub.status.idle":"2021-11-10T01:08:16.817997Z","shell.execute_reply.started":"2021-11-10T01:08:16.315507Z","shell.execute_reply":"2021-11-10T01:08:16.817160Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from keras.layers import Dense, Input, Dropout, GlobalAveragePooling2D, Flatten, Conv2D, BatchNormalization, Activation, MaxPooling2D\nfrom keras.models import Model, Sequential\nfrom keras.optimizers import Adam\n\n# number of possible label values\nnb_classes = 7\n\n# Initialising the CNN\nmodel = Sequential()\n\n# 1 - Convolution\nmodel.add(Conv2D(64,(3,3), padding='same', input_shape=(56, 56,1)))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n# 2nd Convolution layer\nmodel.add(Conv2D(128,(5,5), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n# 3rd Convolution layer\nmodel.add(Conv2D(512,(3,3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n# 4th Convolution layer\nmodel.add(Conv2D(512,(3,3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n# Flattening\nmodel.add(Flatten())\n\n# Fully connected layer 1st layer\nmodel.add(Dense(256))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\n\n# Fully connected layer 2nd layer\nmodel.add(Dense(512))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(nb_classes, activation='softmax'))\n\nprint(model.summary())\n\nopt = Adam(lr=0.0001)\nmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# number of epochs to train the NN\nepochs = 50\n\n# checkpoint to save best model\nfrom keras.callbacks import ModelCheckpoint\n\ncheckpoint = ModelCheckpoint(\"model_weights.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\ncallbacks_list = [checkpoint]\n\nhistory = model.fit_generator(generator=train_generator,\n                                steps_per_epoch=train_generator.n//train_generator.batch_size,\n                                epochs=epochs,\n                                validation_data = validation_generator,\n                                validation_steps = validation_generator.n//validation_generator.batch_size,\n                                callbacks=callbacks_list\n                                )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualise training and testing accuracy and loss\n\ndef plot_results(history):\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    epochs = range(1, len(acc) + 1)\n\n    plt.figure(figsize = (24, 6))\n    plt.subplot(1,2,1)\n    plt.plot(epochs, acc, 'b', label = 'Training Accuracy')\n    plt.plot(epochs, val_acc, 'r', label = 'Validation Accuracy')\n    plt.grid(True)\n    plt.legend()\n    plt.xlabel('Epoch')\n    \n\n\n    plt.subplot(1,2,2)\n    plt.plot(epochs, loss, 'b', label = 'Training Loss')\n    plt.plot(epochs, val_loss, 'r', label = 'Validation Loss')\n    plt.grid(True)\n    plt.legend()\n    plt.xlabel('Epoch')\n    plt.show()\n \n# print best epoch with best accuracy on validation\n\ndef get_best_epcoh(history):\n    valid_acc = history.history['val_accuracy']\n    best_epoch = valid_acc.index(max(valid_acc)) + 1\n    best_acc =  max(valid_acc)\n    print('Best Validation Accuracy Score {:0.5f}, is for epoch {}'.format( best_acc, best_epoch))\n    return best_epoch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_results(history)\nbest_epoch =get_best_epcoh(history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Connecting with openCV","metadata":{"trusted":true}},{"cell_type":"code","source":"json_model = model.to_json()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(json_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nwith open('../input/face-expression-model/model.json','r') as json_file:\n    json.read(json_file)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport cv2\nfrom tensorflow.keras.models import model_from_json","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_json_file = '../input/face-expression-model/model.json'\nmodel_weights_file = '../input/face-expression-model/model_weights.h5'\nwith open(model_json_file, \"r\") as json_file:\n    loaded_model_json = json_file.read()\n    loaded_model = model_from_json(loaded_model_json)\n    loaded_model.load_weights(model_weights_file)\n    \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"face_cascade = cv2.CascadeClassifier('../input/face-expression-model/haarcascade_frontalface_default.xml')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cap = cv2.VideoCapture(0)\nimport copy\n\nwhile True:\n    \n    ret, frame = cap.read()\n    img = copy.deepcopy(frame)\n#     gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n#     faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n#     for (x,y,w,h) in faces:\n#         fc = gray[y:y+h, x:x+w]\n        \n#         roi = cv2.resize(fc, (48,48))\n#         pred = loaded_model.predict(roi[np.newaxis, :, :, np.newaxis])\n#         text_idx=np.argmax(pred)\n#         text_list = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']\n#         if text_idx == 0:\n#             text= text_list[0]\n#         if text_idx == 1:\n#             text= text_list[1]\n#         elif text_idx == 2:\n#             text= text_list[2]\n#         elif text_idx == 3:\n#             text= text_list[3]\n#         elif text_idx == 4:\n#             text= text_list[4]\n#         elif text_idx == 5:\n#             text= text_list[5]\n#         elif text_idx == 6:\n#             text= text_list[6]\n#         cv2.putText(img, text, (x, y-5),\n#            cv2.FONT_HERSHEY_SIMPLEX, 0.45, (255, 0, 255), 2)\n#         img = cv2.rectangle(img, (x,y), (x+w, y+h), (0,0,255), 2)\n            \n    \n    cv2.imshow(\"frame\", img)\n    key = cv2.waitKey(1) & 0xFF\n    if key== ord('q'):\n        break\n    \ncap.release()\ncv2.destroyAllWindows()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}